{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs\n",
    "\n",
    "Using TensorFlow Keras RNN layers (e.g. LSTM, GRU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a time-series dataset. I'll use the \"Plane\" dataset from http://www.cs.ucr.edu/~eamonn/time_series_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fn):\n",
    "    \"\"\"\n",
    "    Load CSV files in UCR time-series data format\n",
    "    \n",
    "    Returns:\n",
    "        data - numpy array with data of shape (num_examples, num_features)\n",
    "        labels - numpy array with labels of shape: (num_examples, 1)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(fn, header=None)\n",
    "    df_data = df.drop(0, axis=1).values.astype(np.float32)\n",
    "    df_labels = df.loc[:, df.columns == 0].values.astype(np.uint8)\n",
    "    return df_data, df_labels\n",
    "\n",
    "train_data, train_labels = load_data(\"Plane/Plane_TRAIN\")\n",
    "test_data, test_labels = load_data(\"Plane/Plane_TEST\")\n",
    "\n",
    "# Information about dataset\n",
    "num_features = 1\n",
    "time_steps = train_data.shape[1]\n",
    "num_classes = len(np.unique(train_labels))\n",
    "data_info = (time_steps, num_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a basic RNN cell. Based on TensorFlow [Keras RNN example](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalRNNCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "        super(MinimalRNNCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      name='kernel')\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='uniform',\n",
    "            name='recurrent_kernel')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = K.dot(inputs, self.kernel)\n",
    "        output = h + K.dot(prev_output, self.recurrent_kernel)\n",
    "        return output, [output]\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Allow loading the init args from the saved model\n",
    "        https://github.com/keras-team/keras/issues/5401#issuecomment-280100357\n",
    "        \"\"\"\n",
    "        config = {'units': self.units}\n",
    "        base_config = super(MinimalRNNCell, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(features, labels, num_classes, batch_size, evaluation=False, buffer_size=5000):\n",
    "    \"\"\"\n",
    "    Get the dataset object for feeding into the model\n",
    "    \n",
    "    If batch_size==None, then one-hot encode but don't batch (evaluation)\n",
    "    If batch_size!=None, then repeat, shuffle, and batch (training)\n",
    "    \"\"\"\n",
    "    def map_func(x, y):\n",
    "        \"\"\" One-hot encode y, convert to appropriate data types \"\"\"\n",
    "        x_out = tf.cast(tf.expand_dims(x,axis=1), tf.float32)\n",
    "        y_out = tf.one_hot(tf.squeeze(tf.cast(y, tf.uint8)), depth=num_classes)\n",
    "        return [x_out, y_out]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    dataset = dataset.map(map_func)\n",
    "    \n",
    "    if evaluation:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "    else:\n",
    "        dataset = dataset.repeat().shuffle(buffer_size).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def get_model(time_steps, num_features, num_classes, layer_type):\n",
    "    \"\"\" Define RNN model \"\"\"\n",
    "    if layer_type == 'lstm':\n",
    "        layer1 = tf.keras.layers.LSTM(128, return_sequences=True)\n",
    "        layer2 = tf.keras.layers.LSTM(128, return_sequences=False)\n",
    "    elif layer_type == 'rnn':\n",
    "        #layer1 = tf.keras.layers.RNN(MinimalRNNCell(128), return_sequences=True)\n",
    "        #layer2 = tf.keras.layers.RNN(MinimalRNNCell(128), return_sequences=False)\n",
    "        layer1 = tf.keras.layers.SimpleRNN(128, return_sequences=True)\n",
    "        layer2 = tf.keras.layers.SimpleRNN(128, return_sequences=False)\n",
    "    elif layer_type == 'gru':\n",
    "        layer1 = tf.keras.layers.GRU(128, return_sequences=True)\n",
    "        layer2 = tf.keras.layers.GRU(128, return_sequences=False)\n",
    "    \n",
    "    x = tf.keras.Input((time_steps,1), dtype=tf.float32)\n",
    "    n = layer1(x)\n",
    "    n = tf.keras.layers.Dropout(0.5)(n)\n",
    "    n = layer2(n)\n",
    "    n = tf.keras.layers.Dropout(0.5)(n)\n",
    "    n = tf.keras.layers.Dense(num_classes)(n)\n",
    "    y = tf.keras.layers.Activation('softmax')(n)\n",
    "    model = tf.keras.Model(x, y)\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def latest_checkpoint(model_file):\n",
    "    \"\"\" Find latest checkpoint -- https://www.tensorflow.org/tutorials/keras/save_and_restore_models \"\"\"\n",
    "    model_path = os.path.dirname(model_file)\n",
    "    #checkpoints = pathlib.Path(model_path).glob(\"*.index\")\n",
    "    checkpoints = pathlib.Path(model_path).glob(\"*.hdf5\")\n",
    "    checkpoints = sorted(checkpoints, key=lambda cp:cp.stat().st_mtime)\n",
    "    #checkpoints = [cp.with_suffix('') for cp in checkpoints]\n",
    "    checkpoints = [cp.with_suffix('.hdf5') for cp in checkpoints]\n",
    "    \n",
    "    if len(checkpoints) > 0:\n",
    "        # Get epoch number from filename\n",
    "        regex = re.compile(r'\\d\\d+')\n",
    "        numbers = [int(x) for x in regex.findall(str(checkpoints[-1]))]\n",
    "        assert len(numbers) == 1, \"Could not determine epoch number from filename since multiple numbers\"\n",
    "        epoch = numbers[0]\n",
    "        \n",
    "        return str(checkpoints[-1]), epoch\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def train(data_info, features, labels,\n",
    "          batch_size=64,\n",
    "          num_epochs=10,\n",
    "          model_file=\"models/{epoch:04d}.hdf5\",\n",
    "          log_dir=\"logs\",\n",
    "          layer_type=\"lstm\"):\n",
    "    \n",
    "    model_path = os.path.dirname(model_file)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    latest, epoch = latest_checkpoint(model_file)\n",
    "\n",
    "    # Data stats\n",
    "    time_steps, num_features, num_classes = data_info\n",
    "\n",
    "    # Get dataset / model\n",
    "    dataset = get_dataset(features, labels, num_classes, batch_size)\n",
    "    \n",
    "    # Load previous weights if found, if not we'll start at epoch 0\n",
    "    if latest is not None:\n",
    "        # Load the entire saved model\n",
    "        model = tf.keras.models.load_model(latest)\n",
    "        \n",
    "        # Alternatively, recreate model and load only the weights from the model file\n",
    "        #model = get_model(time_steps, num_features, num_classes, layer_type)\n",
    "        #model.load_weights(latest)\n",
    "    else:\n",
    "        model = get_model(time_steps, num_features, num_classes, layer_type)\n",
    "        epoch = 0\n",
    "    \n",
    "    # Train\n",
    "    model.fit(dataset, initial_epoch=epoch, epochs=num_epochs, steps_per_epoch=30, callbacks=[\n",
    "        # save_weights_only doesn't work for LSTM apparently, the just-trained model.get_weights()\n",
    "        # don't show up in the model-from-saved-file model.get_weights(), though some are loaded\n",
    "        # like the last dense layer. This is a saving problem definitely since saving the entire\n",
    "        # model and then loading just the weights works fine.\n",
    "        tf.keras.callbacks.ModelCheckpoint(model_file, period=1, verbose=0),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir),\n",
    "        tf.keras.callbacks.TerminateOnNaN()\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate(data_info, features, labels, model=None,\n",
    "             model_file=\"models/{epoch:04d}.hdf5\",\n",
    "             layer_type=\"lstm\",\n",
    "             useTensorFlowDataset=True):\n",
    "    \n",
    "    latest, epoch = latest_checkpoint(model_file)\n",
    "    \n",
    "    # Data stats\n",
    "    time_steps, num_features, num_classes = data_info\n",
    "    \n",
    "    # Get dataset\n",
    "    if useTensorFlowDataset:\n",
    "        dataset = get_dataset(features, labels, num_classes, 1, evaluation=True)\n",
    "    else:\n",
    "        x = np.expand_dims(features,axis=2).astype(np.float32)\n",
    "        y = np.eye(num_classes)[np.squeeze(labels).astype(np.uint8) - 1] # one-hot encode\n",
    "    \n",
    "    # Load weights from last checkpoint if model is not given\n",
    "    if model is None:\n",
    "        assert latest is not None, \"No latest checkpoint to use for evaluation\"\n",
    "        print(\"Loading model from\", latest, \"at epoch\", epoch)\n",
    "        \n",
    "        # Load entire model\n",
    "        model = tf.keras.models.load_model(latest, custom_objects={\n",
    "            'MinimalRNNCell': MinimalRNNCell\n",
    "        })\n",
    "        \n",
    "        # Alternatively, recreate model and load only the weights from the model file\n",
    "        #model = get_model(time_steps, num_features, num_classes, layer_type)\n",
    "        #model.load_weights(latest)\n",
    "    \n",
    "    # Evaluate\n",
    "    if useTensorFlowDataset:\n",
    "        loss, acc = model.evaluate(dataset, steps=len(labels))\n",
    "    else:\n",
    "        loss, acc = model.evaluate(x, y)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: lstm\n",
      "Training model: rnn\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 3s 111ms/step - loss: 1.0375 - acc: 0.4219\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 3s 102ms/step - loss: 0.3351 - acc: 0.6714\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 3s 102ms/step - loss: 0.5267 - acc: 0.6427\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 3s 102ms/step - loss: 0.1490 - acc: 0.7552\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 3s 105ms/step - loss: 0.0561 - acc: 0.7974\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 4s 121ms/step - loss: 0.0213 - acc: 0.8057\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 4s 132ms/step - loss: 0.0601 - acc: 0.7937\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 4s 130ms/step - loss: 0.4213 - acc: 0.6750\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 4s 122ms/step - loss: 0.1652 - acc: 0.7740\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 4s 130ms/step - loss: 0.0820 - acc: 0.7740\n",
      "Training model: gru\n"
     ]
    }
   ],
   "source": [
    "for layer_type in ['lstm', 'rnn', 'gru']:\n",
    "    print(\"Training model:\", layer_type)\n",
    "    tf.reset_default_graph()\n",
    "    K.clear_session()\n",
    "    model = train(data_info, train_data, train_labels,\n",
    "                  model_file=layer_type+\"-models/{epoch:04d}.hdf5\",\n",
    "                  log_dir=layer_type+\"-logs\", layer_type=layer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: lstm\n",
      "Loading model from lstm-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 4s 35ms/step\n",
      "  Train: 0.8095238095238095\n",
      "Loading model from lstm-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 4s 39ms/step\n",
      "  Test: 0.8380952380952381\n",
      "Evaluating model: rnn\n",
      "Loading model from rnn-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 1s 12ms/step\n",
      "  Train: 0.8\n",
      "Loading model from rnn-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 1s 12ms/step\n",
      "  Test: 0.8857142857142857\n",
      "Evaluating model: gru\n",
      "Loading model from gru-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 3s 28ms/step\n",
      "  Train: 0.7238095238095238\n",
      "Loading model from gru-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 3s 28ms/step\n",
      "  Test: 0.7238095238095238\n"
     ]
    }
   ],
   "source": [
    "for layer_type in ['lstm', 'rnn', 'gru']:\n",
    "    print(\"Evaluating model:\", layer_type)\n",
    "    print(\"  Train:\", evaluate(data_info, train_data, train_labels,\n",
    "                               model_file=layer_type+\"-models/{epoch:04d}.hdf5\",\n",
    "                               layer_type=layer_type))\n",
    "    print(\"  Test:\", evaluate(data_info, test_data, test_labels,\n",
    "                              model_file=layer_type+\"-models/{epoch:04d}.hdf5\",\n",
    "                              layer_type=layer_type))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
