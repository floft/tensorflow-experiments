{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs\n",
    "\n",
    "Using TensorFlow Keras RNN layers (e.g. LSTM, GRU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a time-series dataset. I'll use the \"Plane\" dataset from http://www.cs.ucr.edu/~eamonn/time_series_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fn):\n",
    "    \"\"\"\n",
    "    Load CSV files in UCR time-series data format\n",
    "    \n",
    "    Returns:\n",
    "        data - numpy array with data of shape (num_examples, num_features)\n",
    "        labels - numpy array with labels of shape: (num_examples, 1)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(fn, header=None)\n",
    "    df_data = df.drop(0, axis=1).values.astype(np.float32)\n",
    "    df_labels = df.loc[:, df.columns == 0].values.astype(np.uint8)\n",
    "    return df_data, df_labels\n",
    "\n",
    "train_data, train_labels = load_data(\"Plane/Plane_TRAIN\")\n",
    "test_data, test_labels = load_data(\"Plane/Plane_TEST\")\n",
    "\n",
    "# Information about dataset\n",
    "num_features = 1\n",
    "time_steps = train_data.shape[1]\n",
    "num_classes = len(np.unique(train_labels))\n",
    "data_info = (time_steps, num_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a basic RNN cell. Based on TensorFlow [Keras RNN example](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalRNNCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "        super(MinimalRNNCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      name='kernel')\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='uniform',\n",
    "            name='recurrent_kernel')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = K.dot(inputs, self.kernel)\n",
    "        output = h + K.dot(prev_output, self.recurrent_kernel)\n",
    "        return output, [output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(features, labels, num_classes, batch_size, evaluation=False, buffer_size=5000):\n",
    "    \"\"\"\n",
    "    Get the dataset object for feeding into the model\n",
    "    \n",
    "    If batch_size==None, then one-hot encode but don't batch (evaluation)\n",
    "    If batch_size!=None, then repeat, shuffle, and batch (training)\n",
    "    \"\"\"\n",
    "    def map_func(x, y):\n",
    "        \"\"\" One-hot encode y, convert to appropriate data types \"\"\"\n",
    "        x_out = tf.cast(tf.expand_dims(x,axis=1), tf.float32)\n",
    "        y_out = tf.one_hot(tf.squeeze(tf.cast(y, tf.uint8)), depth=num_classes)\n",
    "        return [x_out, y_out]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    dataset = dataset.map(map_func)\n",
    "    \n",
    "    if evaluation:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "    else:\n",
    "        dataset = dataset.repeat().shuffle(buffer_size).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def get_model(time_steps, num_features, num_classes):\n",
    "    \"\"\" Define RNN model \"\"\"\n",
    "    #cell = MinimalRNNCell(32)\n",
    "    x = tf.keras.Input((time_steps,1), dtype=tf.float32)\n",
    "    #n = tf.keras.layers.RNN(cell, return_sequences=False)(x)\n",
    "    n = tf.keras.layers.LSTM(128, return_sequences=True)(x)\n",
    "    n = tf.keras.layers.Dropout(0.5)(n)\n",
    "    n = tf.keras.layers.LSTM(128, return_sequences=False)(n)\n",
    "    n = tf.keras.layers.Dropout(0.5)(n)\n",
    "    n = tf.keras.layers.Dense(num_classes)(n)\n",
    "    y = tf.keras.layers.Activation('softmax')(n)\n",
    "    model = tf.keras.Model(x, y)\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def latest_checkpoint(model_file):\n",
    "    \"\"\" Find latest checkpoint -- https://www.tensorflow.org/tutorials/keras/save_and_restore_models \"\"\"\n",
    "    model_path = os.path.dirname(model_file)\n",
    "    #checkpoints = pathlib.Path(model_path).glob(\"*.index\")\n",
    "    checkpoints = pathlib.Path(model_path).glob(\"*.hdf5\")\n",
    "    checkpoints = sorted(checkpoints, key=lambda cp:cp.stat().st_mtime)\n",
    "    #checkpoints = [cp.with_suffix('') for cp in checkpoints]\n",
    "    checkpoints = [cp.with_suffix('.hdf5') for cp in checkpoints]\n",
    "    \n",
    "    if len(checkpoints) > 0:\n",
    "        # Get epoch number from filename\n",
    "        regex = re.compile(r'\\d\\d+')\n",
    "        numbers = [int(x) for x in regex.findall(str(checkpoints[-1]))]\n",
    "        assert len(numbers) == 1, \"Could not determine epoch number from filename since multiple numbers\"\n",
    "        epoch = numbers[0]\n",
    "        \n",
    "        return str(checkpoints[-1]), epoch\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def train(data_info, features, labels,\n",
    "          batch_size=64,\n",
    "          num_epochs=5,\n",
    "          model_file=\"models/{epoch:04d}.hdf5\",\n",
    "          log_dir=\"logs\"):\n",
    "    \n",
    "    model_path = os.path.dirname(model_file)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    latest, epoch = latest_checkpoint(model_file)\n",
    "\n",
    "    # Data stats\n",
    "    time_steps, num_features, num_classes = data_info\n",
    "\n",
    "    # Get dataset / model\n",
    "    dataset = get_dataset(features, labels, num_classes, batch_size)\n",
    "    \n",
    "    # Load previous weights if found, if not we'll start at epoch 0\n",
    "    if latest is not None:\n",
    "        model = tf.keras.models.load_model(latest)\n",
    "    else:\n",
    "        model = get_model(time_steps, num_features, num_classes)\n",
    "        epoch = 0\n",
    "    \n",
    "    # Train\n",
    "    model.fit(dataset, initial_epoch=epoch, epochs=num_epochs, steps_per_epoch=30, callbacks=[\n",
    "        # save_weights_only doesn't work for LSTM apparently, model.get_weights() before saving\n",
    "        # and after loading differs for LSTM weights but not dense weights, i.e. dense are loaded\n",
    "        # and LSTM are not -- i.e. saving only weights is useless\n",
    "        tf.keras.callbacks.ModelCheckpoint(model_file, period=1, verbose=1),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir),\n",
    "        tf.keras.callbacks.TerminateOnNaN()\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate(data_info, features, labels, model=None,\n",
    "             model_file=\"models/{epoch:04d}.hdf5\",\n",
    "             useTensorFlowDataset=True):\n",
    "    \n",
    "    latest, epoch = latest_checkpoint(model_file)\n",
    "    \n",
    "    # Data stats\n",
    "    time_steps, num_features, num_classes = data_info\n",
    "    \n",
    "    # Get dataset\n",
    "    if useTensorFlowDataset:\n",
    "        dataset = get_dataset(features, labels, num_classes, 1, evaluation=True)\n",
    "    else:\n",
    "        x = np.expand_dims(features,axis=2).astype(np.float32)\n",
    "        y = np.eye(num_classes)[np.squeeze(labels).astype(np.uint8) - 1] # one-hot encode\n",
    "    \n",
    "    # Load weights from last checkpoint if model is not given\n",
    "    if model is None:\n",
    "        assert latest is not None, \"No latest checkpoint to use for evaluation\"\n",
    "        print(\"Loading model from\", latest, \"at epoch\", epoch)\n",
    "        model = tf.keras.models.load_model(latest)\n",
    "    \n",
    "    # Evaluate\n",
    "    if useTensorFlowDataset:\n",
    "        loss, acc = model.evaluate(dataset, steps=len(labels))\n",
    "    else:\n",
    "        loss, acc = model.evaluate(x, y)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30/30 [==============================] - 13s 418ms/step - loss: 1.3913 - acc: 0.2323\n",
      "\n",
      "Epoch 00001: saving model to models/0001.hdf5\n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 14s 460ms/step - loss: 0.8626 - acc: 0.4771\n",
      "\n",
      "Epoch 00002: saving model to models/0002.hdf5\n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 16s 528ms/step - loss: 0.3525 - acc: 0.6714\n",
      "\n",
      "Epoch 00003: saving model to models/0003.hdf5\n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 16s 533ms/step - loss: 0.1995 - acc: 0.7328\n",
      "\n",
      "Epoch 00004: saving model to models/0004.hdf5\n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 15s 503ms/step - loss: 0.1120 - acc: 0.7578\n",
      "\n",
      "Epoch 00005: saving model to models/0005.hdf5\n",
      "105/105 [==============================] - 4s 38ms/step\n",
      "Accuracy from trained model: 0.819047619047619\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "K.clear_session()\n",
    "model = train(data_info, train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models/0005.hdf5 at epoch 5\n",
      "105/105 [==============================] - 3s 27ms/step\n",
      "Accuracy from loading weights on training data: 0.7714285714285715\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy from loading weights on training data:\", evaluate(data_info, train_data, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models/0005.hdf5 at epoch 5\n",
      "105/105 [==============================] - 3s 27ms/step\n",
      "Accuracy from loading weights: 0.819047619047619\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy from loading weights:\", evaluate(data_info, test_data, test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
