{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs -- VRNN, LSTM, GRU, SimpleRNN\n",
    "\n",
    "Using TensorFlow Keras RNN layers (e.g. LSTM, GRU). The main purpose is to implement VRNN and compare with just LSTM, GRU, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import activations\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a time-series dataset. I'll use the \"Plane\" dataset from http://www.cs.ucr.edu/~eamonn/time_series_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fn):\n",
    "    \"\"\"\n",
    "    Load CSV files in UCR time-series data format\n",
    "    \n",
    "    Returns:\n",
    "        data - numpy array with data of shape (num_examples, num_features)\n",
    "        labels - numpy array with labels of shape: (num_examples, 1)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(fn, header=None)\n",
    "    df_data = df.drop(0, axis=1).values.astype(np.float32)\n",
    "    df_labels = df.loc[:, df.columns == 0].values.astype(np.uint8)\n",
    "    return df_data, df_labels\n",
    "\n",
    "train_data, train_labels = load_data(\"Plane/Plane_TRAIN\")\n",
    "test_data, test_labels = load_data(\"Plane/Plane_TEST\")\n",
    "\n",
    "# Information about dataset\n",
    "num_features = 1\n",
    "time_steps = train_data.shape[1]\n",
    "num_classes = len(np.unique(train_labels))\n",
    "data_info = (time_steps, num_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a basic RNN cell. Based on TensorFlow [Keras RNN example](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN) and [SimpleRNN](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/keras/layers/recurrent.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalRNNCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "        self.activation = activations.get('tanh')\n",
    "        super(MinimalRNNCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      name='kernel')\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='orthogonal',\n",
    "            name='recurrent_kernel')\n",
    "        # https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/keras/engine/base_layer.py\n",
    "        # says to do this not \"self.built = True\" like was given in the example\n",
    "        super(MinimalRNNCell, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = K.dot(inputs, self.kernel)\n",
    "        output = h + K.dot(prev_output, self.recurrent_kernel)\n",
    "        output = self.activation(output)\n",
    "        return output, [output]\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Allow loading the init args from the saved model\n",
    "        https://github.com/keras-team/keras/issues/5401#issuecomment-280100357\n",
    "        \"\"\"\n",
    "        config = {'units': self.units}\n",
    "        base_config = super(MinimalRNNCell, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing VRNN. Based on:\n",
    " - https://github.com/phreeza/tensorflow-vrnn/blob/master/model_vrnn.py\n",
    " - https://github.com/kimkilho/tensorflow-vrnn/blob/master/cell.py\n",
    " - https://github.com/kimkilho/tensorflow-vrnn/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNNCell(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    VRNN cell\n",
    "    \n",
    "    Usage:\n",
    "        cell = VRNNCell(h_dim, z_dim)\n",
    "        net = tf.keras.layers.RNN(cell)(net)\n",
    "    \"\"\"\n",
    "    def __init__(self, x_dim, h_dim, z_dim, batch_size, **kwargs):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Dimensions of x input, hidden layers, latent variable (z)\n",
    "        self.n_x = x_dim\n",
    "        self.n_h = h_dim\n",
    "        self.n_z = z_dim\n",
    "        \n",
    "        # Dimensions of phi(z)\n",
    "        self.n_x_1 = x_dim\n",
    "        self.n_z_1 = z_dim\n",
    "        \n",
    "        # Dimensions of encoder, decoder, and prior\n",
    "        self.n_enc_hidden = z_dim\n",
    "        self.n_dec_hidden = x_dim\n",
    "        self.n_prior_hidden = z_dim\n",
    "        \n",
    "        # What cell we're going to use internally for the RNN\n",
    "        self.cell = tf.keras.layers.LSTMCell(self.n_h,\n",
    "             input_shape=(None, self.n_dec_hidden+self.n_z_1))\n",
    "        \n",
    "        super(VRNNCell, self).__init__(**kwargs)\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        # Note: first two are the state of the LSTM\n",
    "        return (self.n_h, self.n_h,\n",
    "                self.n_z, self.n_z,\n",
    "                self.n_x, self.n_x,\n",
    "                self.n_z, self.n_z)\n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.n_h\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Input: previous hidden state\n",
    "        self.prior_h = self.add_weight(\n",
    "            shape=(self.n_h, self.n_prior_hidden), initializer='glorot_uniform', name='prior_hidden')\n",
    "        self.prior_mu = self.add_weight(\n",
    "            shape=(self.n_prior_hidden, self.n_z), initializer='glorot_uniform', name='prior_mu')\n",
    "        self.prior_sigma = self.add_weight(\n",
    "            shape=(self.n_prior_hidden, self.n_z), initializer='glorot_uniform', name='prior_sigma')\n",
    "        \n",
    "        self.prior_h_b = self.add_weight(\n",
    "            shape=(self.n_prior_hidden,), initializer='constant', name='prior_hidden_b')\n",
    "        self.prior_mu_b = self.add_weight(\n",
    "            shape=(self.n_z,), initializer='constant', name='prior_mu_b')\n",
    "        self.prior_sigma_b = self.add_weight(\n",
    "            shape=(self.n_z,), initializer='constant', name='prior_sigma_b')\n",
    "        \n",
    "        # Input: x\n",
    "        self.x_1 = self.add_weight(\n",
    "            shape=(self.n_x, self.n_x_1), initializer='glorot_uniform', name='phi_x')\n",
    "        \n",
    "        self.x_1_b = self.add_weight(\n",
    "            shape=(self.n_x_1,), initializer='constant', name='phi_x_b')\n",
    "        \n",
    "        # Input: x and previous hidden state\n",
    "        self.encoder_h = self.add_weight(\n",
    "            shape=(self.n_x_1+self.n_h, self.n_enc_hidden), initializer='glorot_uniform', name='encoder_hidden')\n",
    "        self.encoder_mu = self.add_weight(\n",
    "            shape=(self.n_enc_hidden, self.n_z), initializer='glorot_uniform', name='encoder_mu')\n",
    "        self.encoder_sigma = self.add_weight(\n",
    "            shape=(self.n_enc_hidden, self.n_z), initializer='glorot_uniform', name='encoder_sigma')\n",
    "        \n",
    "        self.encoder_h_b = self.add_weight(\n",
    "            shape=(self.n_enc_hidden,), initializer='constant', name='encoder_hidden_b')\n",
    "        self.encoder_mu_b = self.add_weight(\n",
    "            shape=(self.n_z,), initializer='constant', name='encoder_mu_b')\n",
    "        self.encoder_sigma_b = self.add_weight(\n",
    "            shape=(self.n_z,), initializer='constant', name='encoder_sigma_b')\n",
    "        \n",
    "        # Input: z = enc_sigma*eps + enc_mu -- i.e. reparameterization trick\n",
    "        self.z_1 = self.add_weight(\n",
    "            shape=(self.n_z, self.n_z_1), initializer='glorot_uniform', name='phi_z')\n",
    "        \n",
    "        self.z_1_b = self.add_weight(\n",
    "            shape=(self.n_z_1,), initializer='constant', name='phi_z_b')\n",
    "        \n",
    "        # Input: latent variable (z) and previous hidden state\n",
    "        self.decoder_h = self.add_weight(\n",
    "            shape=(self.n_z+self.n_h, self.n_dec_hidden), initializer='glorot_uniform', name='decoder_hidden')\n",
    "        self.decoder_mu = self.add_weight(\n",
    "            shape=(self.n_dec_hidden, self.n_x), initializer='glorot_uniform', name='decoder_mu')\n",
    "        self.decoder_sigma = self.add_weight(\n",
    "            shape=(self.n_dec_hidden, self.n_x), initializer='glorot_uniform', name='decoder_sigma')\n",
    "        \n",
    "        self.decoder_h_b = self.add_weight(\n",
    "            shape=(self.n_dec_hidden,), initializer='constant', name='decoder_hidden_b')\n",
    "        self.decoder_mu_b = self.add_weight(\n",
    "            shape=(self.n_x,), initializer='constant', name='decoder_mu_b')\n",
    "        self.decoder_sigma_b = self.add_weight(\n",
    "            shape=(self.n_x,), initializer='constant', name='decoder_sigma_b')\n",
    "        \n",
    "        super(VRNNCell, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        h = states[0]\n",
    "        c = states[1] # only passed to the LSTM\n",
    "        \n",
    "        # Input: previous hidden state (h)\n",
    "        prior_h = K.relu(K.dot(h, self.prior_h) + self.prior_h_b)\n",
    "        prior_mu = K.dot(prior_h, self.prior_mu) + self.prior_mu_b\n",
    "        prior_sigma = K.softplus(K.dot(prior_h, self.prior_sigma) + self.prior_sigma_b) # >= 0\n",
    "        \n",
    "        # Input: x\n",
    "        x_1 = K.relu(K.dot(inputs, self.x_1) + self.x_1_b) # >= 0\n",
    "        \n",
    "        # Input: x and previous hidden state\n",
    "        encoder_input = K.concatenate((x_1, h), 1)\n",
    "        encoder_h = K.relu(K.dot(encoder_input, self.encoder_h) + self.encoder_h_b)\n",
    "        encoder_mu = K.dot(encoder_h, self.encoder_mu) + self.encoder_mu_b\n",
    "        encoder_sigma = K.softplus(K.dot(encoder_h, self.encoder_sigma) + self.encoder_sigma_b)\n",
    "        \n",
    "        # Input: z = enc_sigma*eps + enc_mu -- i.e. reparameterization trick\n",
    "        #eps = K.random_normal((self.batch_size, self.n_z), dtype=tf.float32)\n",
    "        eps = K.random_normal((self.batch_size, self.n_z), dtype=tf.float32)\n",
    "        z = encoder_sigma*eps + encoder_mu\n",
    "        z_1 = K.relu(K.dot(z, self.z_1) + self.z_1_b)\n",
    "        \n",
    "        # Input: latent variable (z) and previous hidden state\n",
    "        decoder_input = K.concatenate((z_1, h), 1)\n",
    "        decoder_h = K.relu(K.dot(decoder_input, self.decoder_h) + self.decoder_h_b)\n",
    "        decoder_mu = K.dot(decoder_h, self.decoder_mu) + self.decoder_mu_b\n",
    "        decoder_sigma = K.softplus(K.dot(decoder_h, self.decoder_sigma) + self.decoder_sigma_b)\n",
    "        \n",
    "        # Pass to cell (e.g. LSTM). Note that the LSTM has both \"h\" and \"c\" that are combined\n",
    "        # into the same next state vector. We'll combine them together to pass in and split them\n",
    "        # back out after the LSTM returns the next state.\n",
    "        rnn_cell_input = K.concatenate((x_1, z_1), 1)\n",
    "        output, (h_next, c_next) = self.cell(rnn_cell_input, [h, c])\n",
    "        \n",
    "        # VRNN state\n",
    "        next_state = (\n",
    "            h_next,\n",
    "            c_next,\n",
    "            encoder_mu,\n",
    "            encoder_sigma,\n",
    "            decoder_mu,\n",
    "            decoder_sigma,\n",
    "            prior_mu,\n",
    "            prior_sigma,\n",
    "        )\n",
    "        \n",
    "        return output, next_state\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\" Save cell config to the model file \"\"\"\n",
    "        config = {\n",
    "            'batch_size': self.batch_size,\n",
    "            'x_dim': self.n_x,\n",
    "            'h_dim': self.n_h,\n",
    "            'z_dim': self.n_z,\n",
    "        }\n",
    "        base_config = super(VRNNCell, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(features, labels, num_classes, batch_size, evaluation=False, buffer_size=5000):\n",
    "    \"\"\"\n",
    "    Get the dataset object for feeding into the model\n",
    "    \n",
    "    If batch_size==None, then one-hot encode but don't batch (evaluation)\n",
    "    If batch_size!=None, then repeat, shuffle, and batch (training)\n",
    "    \"\"\"\n",
    "    def map_func(x, y):\n",
    "        \"\"\" One-hot encode y, convert to appropriate data types \"\"\"\n",
    "        x_out = tf.cast(tf.expand_dims(x,axis=1), tf.float32)\n",
    "        y_out = tf.one_hot(tf.squeeze(tf.cast(y, tf.uint8)), depth=num_classes)\n",
    "        return [x_out, y_out]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    dataset = dataset.map(map_func)\n",
    "    \n",
    "    if evaluation:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "    else:\n",
    "        dataset = dataset.repeat().shuffle(buffer_size).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def get_model(time_steps, num_features, num_classes, batch_size, layer_type):\n",
    "    \"\"\" Define RNN model \"\"\"\n",
    "    if layer_type == 'lstm':\n",
    "        layer1 = tf.keras.layers.LSTM(128, return_sequences=True)\n",
    "        layer2 = tf.keras.layers.LSTM(128, return_sequences=False)\n",
    "    elif layer_type == 'rnn':\n",
    "        layer1 = tf.keras.layers.SimpleRNN(128, return_sequences=True)\n",
    "        layer2 = tf.keras.layers.SimpleRNN(128, return_sequences=False)\n",
    "    elif layer_type == 'custom_rnn':\n",
    "        layer1 = tf.keras.layers.RNN(MinimalRNNCell(128), return_sequences=True)\n",
    "        layer2 = tf.keras.layers.RNN(MinimalRNNCell(128), return_sequences=False)\n",
    "    elif layer_type == 'vrnn':\n",
    "        input_size = num_features # x input features\n",
    "        layer1 = tf.keras.layers.RNN(VRNNCell(input_size, 128, 32, batch_size), return_sequences=True)\n",
    "        input_size = 128 # size of previous layer output\n",
    "        layer2 = tf.keras.layers.RNN(VRNNCell(input_size, 128, 32, batch_size), return_sequences=False)\n",
    "    elif layer_type == 'gru':\n",
    "        layer1 = tf.keras.layers.GRU(128, return_sequences=True)\n",
    "        layer2 = tf.keras.layers.GRU(128, return_sequences=False)\n",
    "    \n",
    "    x = tf.keras.Input((time_steps,1), dtype=tf.float32)\n",
    "    n = layer1(x)\n",
    "    n = tf.keras.layers.Dropout(0.5)(n)\n",
    "    n = layer2(n)\n",
    "    n = tf.keras.layers.Dropout(0.5)(n)\n",
    "    n = tf.keras.layers.Dense(num_classes)(n)\n",
    "    y = tf.keras.layers.Activation('softmax')(n)\n",
    "    model = tf.keras.Model(x, y)\n",
    "    \n",
    "    if layer_type == 'vrnn':\n",
    "        model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                      #optimizer=tf.keras.optimizers.Adam(), # gives gradient None error\n",
    "                      optimizer=tf.train.AdamOptimizer(),\n",
    "                      metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                      optimizer=tf.keras.optimizers.Adam(),\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def latest_checkpoint(model_file):\n",
    "    \"\"\"\n",
    "    Find latest checkpoint\n",
    "    https://www.tensorflow.org/tutorials/keras/save_and_restore_models\n",
    "    \"\"\"\n",
    "    model_path = os.path.dirname(model_file)\n",
    "    checkpoints = pathlib.Path(model_path).glob(\"*.hdf5\")\n",
    "    checkpoints = sorted(checkpoints, key=lambda cp:cp.stat().st_mtime)\n",
    "    checkpoints = [cp.with_suffix('.hdf5') for cp in checkpoints]\n",
    "    \n",
    "    if len(checkpoints) > 0:\n",
    "        # Get epoch number from filename\n",
    "        regex = re.compile(r'\\d\\d+')\n",
    "        numbers = [int(x) for x in regex.findall(str(checkpoints[-1]))]\n",
    "        assert len(numbers) == 1, \"Could not determine epoch number from filename since multiple numbers\"\n",
    "        epoch = numbers[0]\n",
    "        \n",
    "        return str(checkpoints[-1]), epoch\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def train(data_info, features, labels,\n",
    "          batch_size=64,\n",
    "          num_epochs=10,\n",
    "          model_file=\"models/{epoch:04d}.hdf5\",\n",
    "          log_dir=\"logs\",\n",
    "          layer_type=\"lstm\"):\n",
    "    \n",
    "    model_path = os.path.dirname(model_file)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    latest, epoch = latest_checkpoint(model_file)\n",
    "\n",
    "    # Data stats\n",
    "    time_steps, num_features, num_classes = data_info\n",
    "\n",
    "    # Get dataset / model\n",
    "    dataset = get_dataset(features, labels, num_classes, batch_size)\n",
    "    \n",
    "    # Load previous weights if found, if not we'll start at epoch 0\n",
    "    if latest is not None:\n",
    "        # Load the entire saved model\n",
    "        #model = tf.keras.models.load_model(latest, custom_objects={\n",
    "        #    'MinimalRNNCell': MinimalRNNCell,\n",
    "        #    'VRNNCell': VRNNCell,\n",
    "        #})\n",
    "        \n",
    "        # Alternatively, recreate model and load only the weights from the model file\n",
    "        model = get_model(time_steps, num_features, num_classes, batch_size, layer_type)\n",
    "        model.load_weights(latest)\n",
    "    else:\n",
    "        model = get_model(time_steps, num_features, num_classes, batch_size, layer_type)\n",
    "        epoch = 0\n",
    "    \n",
    "    # Train\n",
    "    model.fit(dataset, initial_epoch=epoch, epochs=num_epochs, steps_per_epoch=30, callbacks=[\n",
    "        # save_weights_only doesn't work for LSTM apparently, the just-trained model.get_weights()\n",
    "        # don't show up in the model-from-saved-file model.get_weights(), though some are loaded\n",
    "        # like the last dense layer. This is a saving problem definitely since saving the entire\n",
    "        # model and then loading just the weights works fine.\n",
    "        tf.keras.callbacks.ModelCheckpoint(model_file, period=1, verbose=0),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir),\n",
    "        tf.keras.callbacks.TerminateOnNaN()\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate(data_info, features, labels, model=None,\n",
    "             model_file=\"models/{epoch:04d}.hdf5\",\n",
    "             layer_type=\"lstm\",\n",
    "             batch_size=64,\n",
    "             useTensorFlowDataset=True):\n",
    "    \n",
    "    latest, epoch = latest_checkpoint(model_file)\n",
    "    \n",
    "    # Data stats\n",
    "    time_steps, num_features, num_classes = data_info\n",
    "    \n",
    "    # Get dataset\n",
    "    if useTensorFlowDataset:\n",
    "        dataset = get_dataset(features, labels, num_classes, 1, evaluation=True)\n",
    "    else:\n",
    "        x = np.expand_dims(features,axis=2).astype(np.float32)\n",
    "        y = np.eye(num_classes)[np.squeeze(labels).astype(np.uint8) - 1] # one-hot encode\n",
    "    \n",
    "    # Load weights from last checkpoint if model is not given\n",
    "    if model is None:\n",
    "        assert latest is not None, \"No latest checkpoint to use for evaluation\"\n",
    "        print(\"Loading model from\", latest, \"at epoch\", epoch)\n",
    "        \n",
    "        # Load entire model\n",
    "        #model = tf.keras.models.load_model(latest, custom_objects={\n",
    "        #    'MinimalRNNCell': MinimalRNNCell,\n",
    "        #    'VRNNCell': VRNNCell,\n",
    "        #})\n",
    "        \n",
    "        # Alternatively, recreate model and load only the weights from the model file\n",
    "        model = get_model(time_steps, num_features, num_classes, batch_size, layer_type)\n",
    "        model.load_weights(latest)\n",
    "    \n",
    "    # Evaluate\n",
    "    if useTensorFlowDataset:\n",
    "        loss, acc = model.evaluate(dataset, steps=len(labels))\n",
    "    else:\n",
    "        loss, acc = model.evaluate(x, y)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: vrnn\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 12s 407ms/step - loss: 1.5508 - acc: 0.1536\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 12s 415ms/step - loss: 1.4605 - acc: 0.1552\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 15s 487ms/step - loss: 1.4385 - acc: 0.1583\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 13s 442ms/step - loss: 1.4965 - acc: 0.1422\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 13s 435ms/step - loss: 1.4693 - acc: 0.1750\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 13s 440ms/step - loss: 1.4604 - acc: 0.1661\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 13s 435ms/step - loss: 1.4350 - acc: 0.1510\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 14s 455ms/step - loss: 1.4501 - acc: 0.1620\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 15s 498ms/step - loss: 1.4541 - acc: 0.1531\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 15s 502ms/step - loss: 1.4482 - acc: 0.1693\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
     ]
    }
   ],
   "source": [
    "#for layer_type in ['lstm', 'rnn', 'gru', 'custom_rnn', 'vrnn']:\n",
    "for layer_type in ['vrnn']:\n",
    "    print(\"Training model:\", layer_type)\n",
    "    tf.reset_default_graph()\n",
    "    K.clear_session()\n",
    "    model = train(data_info, train_data, train_labels,\n",
    "                  model_file=layer_type+\"-models/{epoch:04d}.hdf5\",\n",
    "                  log_dir=layer_type+\"-logs\", layer_type=layer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: lstm\n",
      "Loading model from lstm-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 4s 38ms/step\n",
      "  Train: 0.8095238095238095\n",
      "Loading model from lstm-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 4s 37ms/step\n",
      "  Test: 0.8380952380952381\n",
      "Evaluating model: rnn\n",
      "Loading model from rnn-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 1s 10ms/step\n",
      "  Train: 0.8\n",
      "Loading model from rnn-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 1s 12ms/step\n",
      "  Test: 0.8857142857142857\n",
      "Evaluating model: gru\n",
      "Loading model from gru-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 3s 28ms/step\n",
      "  Train: 0.7238095238095238\n",
      "Loading model from gru-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 3s 26ms/step\n",
      "  Test: 0.7238095238095238\n",
      "Evaluating model: custom_rnn\n",
      "Loading model from custom_rnn-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 1s 10ms/step\n",
      "  Train: 0.8095238095238095\n",
      "Loading model from custom_rnn-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 1s 10ms/step\n",
      "  Test: 0.8857142857142857\n",
      "Evaluating model: vrnn\n",
      "Loading model from vrnn-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 5s 52ms/step\n",
      "  Train: 0.1523809523809524\n",
      "Loading model from vrnn-models/0010.hdf5 at epoch 10\n",
      "105/105 [==============================] - 5s 51ms/step\n",
      "  Test: 0.11428571428571428\n"
     ]
    }
   ],
   "source": [
    "for layer_type in ['lstm', 'rnn', 'gru', 'custom_rnn', 'vrnn']:\n",
    "    print(\"Evaluating model:\", layer_type)\n",
    "    print(\"  Train:\", evaluate(data_info, train_data, train_labels,\n",
    "                               model_file=layer_type+\"-models/{epoch:04d}.hdf5\",\n",
    "                               layer_type=layer_type))\n",
    "    print(\"  Test:\", evaluate(data_info, test_data, test_labels,\n",
    "                              model_file=layer_type+\"-models/{epoch:04d}.hdf5\",\n",
    "                              layer_type=layer_type))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
